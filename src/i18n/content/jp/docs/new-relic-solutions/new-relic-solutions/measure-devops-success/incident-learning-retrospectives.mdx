---
title: 事件の学習。回顧録
tags:
  - New Relic solutions
  - New Relic solutions
  - Measure DevOps success
metaDescription: 'Incident orchestration is the alignment of teams, tools, and processes to prepare for incidents and outages in your software.'
translationType: machine
---

インシデントから学び、問題の再利用を止めるためにチームを装備する。 **Blameless Post-Mortem** という言葉を聞いたことがあるかもしれませんが、ここではそのための方法と理由を説明します。問題から学ぶためのプロセスを構築することで、組織は既存のKPIやインシデント対応パターンを改善し、新たな課題が浮上した場合にも適応できるようになります。目標は、まず学び、次に解決することです。

確かに停電はしましたが、今はみんなに注目されています。それを利用してください。

## 前提条件

このチュートリアルを始める前に、以下のNew Relicチュートリアルを必ず完了してください。

* [目標とベースラインの設定](/docs/using-new-relic/welcome-new-relic/optimize-your-cloud-native-environment/establish-objectives-baselines-define-team-slos)
* [インシデントオーケストレーション](/docs/using-new-relic/welcome-new-relic/measure-devops-success/incident-orchestration-align-teams-tools-processes)

## 1.ポストモーテムプロセスの確立 [#establish-process]

その目的は、注目すべきインシデントが発生するたびに、技術的、組織的、プロセス的なフォローアップアクションを特定することです。

**誰が何をしたか** ではなく、 **何が起こったか** に焦点を当てることで、より多くの成功を収めることができました。といった質問をしてみましょう。

* この問題はどのようにして知らされたのか？
* もっと早く問題を発見できたのではないか？
* 事件解決に必要な情報に簡単にアクセスできたか？
* コンピューターがやるべき仕事を人間がやったのはどこか？

続きを読む [非難されないレトロスペクティブを開催する方法と理由](https://blog.newrelic.com/technology/blameless-retrospectives/) 。このプロセスは、チーム間で同じであることが理想です。

を含むドキュメントを作成します。

* そのきっかけとなる出来事。
* インシデントにつながる要因
* 是正措置の手順とその結果の時系列と概要（何が良かったのかを必ず記載してください。）
* 可能であれば、ユーザー体験や金銭的損失など、ビジネスへの影響を測るもの。
* 再発防止のためのシステムまたは機能の改善に関する推奨事項。
* プロセスやコミュニケーションの改善を提案
* レトロ後のアクションの所有者

**根本的な原因がない？**

なぜ **根本原因** がこの死後の報告書に含まれていないのですか？根本原因とは、複雑なシステムの中で、ある結果に対する単一の原因を見つけることが可能であり、またそれが有用であることを意味しています。私たちは単純で実行可能な説明を求めますが、現在のシステムは複雑すぎて単純な答えが得られないため、この概念は魅力的です。しかし、考え方を広げれば、もっと重要で影響力のある道があるかもしれないのに、それを探さずに、狭い選択肢の中から一つの道を選んでしまうという点では、有害です。

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        死後レポートの例
      </th>

      <th>
        コメント
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        日付
      </td>

      <td>
        2018年3月1日
      </td>
    </tr>

    <tr>
      <td>
        要旨
      </td>

      <td>
        午後1時45分頃から午後2時30分頃まで、ユーザーはカートに品目を追加できなかった。これにより、インシデント期間中にチェックアウトができなかった。
      </td>
    </tr>

    <tr>
      <td>
        トリガーイベント
      </td>

      <td>
        商品詳細ページのCSSルールに変更が加えられ、 **Add to cart** ボタンが事実上無効になっていることが判明しました。
      </td>
    </tr>

    <tr>
      <td>
        時系列
      </td>

      <td>
        * 1:50PM: `Successful checkouts< 10 for 5 minutes` alert triggered; assigned to Alice.
        * 午後1時55分。電子商取引チームのダッシュボードを確認した結果、アリスは、ボブによるデプロイの直後に閾値が破られたと判断しました。AliceはBobにその旨を伝えました。
        * 午後2時：AliceとBobはトラブルシューティングを開始。本番環境での問題再現に成功。
        * 2:20PM: Bobは、商品詳細ページのCSSを変更したことで、 **Add to cart** ボタンが無効になったと判断しました。修正プログラムを適用しました。
        * 午後2時30分：機能が回復し、インシデントが解決。
      </td>
    </tr>

    <tr>
      <td>
        影響
      </td>

      <td>
        インシデントの間、チェックアウトが完了しなかった。この期間の木曜日の通常の売上は30,000ドル。
      </td>
    </tr>

    <tr>
      <td>
        推奨事項
      </td>

      <td>
        [New Relic Synthetics](/docs/synthetics/new-relic-synthetics/getting-started/introduction-new-relic-synthetics) の実装については、以前から検討していました。もしチェックアウトプロセスにシンセティックチェックを導入していたら、この問題はすぐに発見されていたでしょう。また、フロントエンドのウェブアプリでは、より徹底したユニットテストを実施する必要があります。
      </td>
    </tr>

    <tr>
      <td>
        次のステップ
      </td>

      <td>
        アリスは次のスプリントで合成チェックを実装する。Raviのチームはより多くのユニットテストの作成を検討します。
      </td>
    </tr>
  </tbody>
</table>

最初のうちは、パーミッションやアクセスの修正、機器の欠落、特定のアラートの調整など、明らかなフォローアップがレトロによって生成されます。時間が経つにつれ、レトロはより大きなフォローアップを明らかにします。私たちはこれらを **介入** と呼んでいます。介入（GUIDの使用から整数への変更など）を急いで実行する前に、これらの変更の影響をマッピングするための時間が必要です。大規模な変更を行うことで、以前のインシデントが発生しないようにすることはできるかもしれませんが、他にどのような問題を導入したり、リスクを負うことになるのでしょうか。すべてのインシデントに予防措置が必要なわけではありません。ある程度のリスクを受け入れ、その影響を軽減することは合理的です。

## 2.研究しすぎない [#over-research]

過剰なオーバーヘッドによる利益の減少を防ぐために、どのようなインシデントに詳細な分析が必要かというガイドラインを設定する。ハードウェアに障害が発生し、そのハードウェアを交換したら、おそらくそれで終わりです。インシデントがあまりにも複雑で（あるいは感情的な熱を帯びていて）、明確なフォローアップ行動が取れない可能性がある場合は、正式なレビューを行わないという選択もあります。むしろ、ミーティングを開いて、出来事についてオープンに議論しましょう。メモを取りますが、後付けのレポートを作る必要はありません。

## 3.モニタリングのチューニング [#monitoring]

大規模なインシデントの後にモニタリングとアラートを監査し、KPIとしきい値を調整し、検出までの時間を改善し、アラートのノイズを低減する。 [プロアクティブアラートの設定](/docs/using-new-relic/welcome-new-relic/optimize-your-cloud-native-environment/set-proactive-alerts-align-teams-tools-processes-incident-response) パターンを見直します。アラートを以下のように整備します。

1. チーム内のページの頻度を測定します。

2. ポリシーの衛生状態を維持する。一貫性のある名前の付け方をするなど、ちょっとしたことで大きな違いが生まれます。

3. パフォーマンスの向上に合わせてアラートを調整

4. AIを活用してアラートを微調整する

   * [](/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/get-started-incident-intelligence#)最も正確なインシデントの相関関係を得るために、システムをトレーニングします。
   * [ミューティングルール](/docs/alerts-applied-intelligence/new-relic-alerts/alert-notifications/muting-rules-suppress-notifications) を使って、必要のないアラートを消します。
   * [システムが検知した異常に対するアラートの設定](/docs/alerts-applied-intelligence/applied-intelligence/proactive-detection/proactive-detection-applied-intelligence).

5. 条件に応じたランブックの作成アラート用のランブックには

   * このアラートが作成された理由の説明。
   * アラートが監視しているもの
   * そのアラートが示す、システムの状態。
   * オンコールエンジニアがトリアージを開始するための初期ステップ。

[グルーミングとアラートのチューニングについて詳しくはこちら](https://blog.newrelic.com/engineering/prevent-alert-fatigue/).

## 4.インシデントリポジトリの作成 [#triage]

インシデントのポストモーテム文書やその他のインシデントの成果物を一元化し、検索可能なリポジトリを作成し、組織内で得られた教訓にアクセスできるようにする。これらにアクセスできるのは、組織内のごく一部の人だけでしょう。しかし、これらのレポートを十分に作成できるようになれば、この情報の使用を自動化することができるようになります。

## アウトカム [#postmortem]

インシデントからの情報収集を継続することで、SLOの遵守率が向上し、重大インシデントが減少し、開発者の満足度が向上し、従業員の離職率が低下するはずです。